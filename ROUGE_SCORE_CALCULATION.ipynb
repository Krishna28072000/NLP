{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0cc7c18-cf21-45ff-95f6-433757266e98",
   "metadata": {},
   "source": [
    "# LLM Response evaluation using ROUGE SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48607fdc",
   "metadata": {},
   "source": [
    "**The ROUGE score (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of text summarization and machine-generated text against a reference or ground truth**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce9768",
   "metadata": {},
   "source": [
    "* Objective : To evaluate rogue score of the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bc48ea-f7db-4748-b6fa-aab7260fd928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: absl-py in c:\\users\\gkris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\gkris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\gkris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\gkris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\gkris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\gkris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gkris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk->rouge-score) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gkris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk->rouge-score) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\gkris\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24970 sha256=7f5600c1ef1f2a922a34b22c89443822d3eff48375f0acb3698cb18fe18bfcc0\n",
      "  Stored in directory: c:\\users\\gkris\\appdata\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d4b09-9744-4ef0-ad88-825186816d65",
   "metadata": {},
   "source": [
    "* Rogue - Score is mainly depended upon these factors.\n",
    "* Precision: Fraction of matched n-grams in the generated text.\n",
    "* Recall: Fraction of matched n-grams in the reference text.\n",
    "* F1-Score: Harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c590d6",
   "metadata": {},
   "source": [
    "# Rouge - Score Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88992128-f683-4969-842a-e4f725efa426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Precision: 0.8182, Recall: 0.6923, F1: 0.7500\n",
      "rouge2: Precision: 0.7000, Recall: 0.5833, F1: 0.6364\n",
      "rouge3: Precision: 0.5556, Recall: 0.4545, F1: 0.5000\n",
      "rougeL: Precision: 0.8182, Recall: 0.6923, F1: 0.7500\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Reference and generated summaries\n",
    "reference = \"The cat sat on the mat, watching tom and jerry cartoon in television\"\n",
    "generated = \"The cat is sitting on the mat, watching tom and jerry\"\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "#scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "#scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "#scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rouge3'], use_stemmer=True)\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2','rouge3', 'rougeL'],use_stemmer=True)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "scores = scorer.score(reference, generated)\n",
    "\n",
    "# Print results\n",
    "for rouge_type, score in scores.items():\n",
    "    print(f\"{rouge_type}: Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, F1: {score.fmeasure:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8abeb418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Precision: 1.0000, Recall: 0.8333, F1: 0.9091\n",
      "rouge2: Precision: 0.7500, Recall: 0.6000, F1: 0.6667\n",
      "rouge3: Precision: 0.6667, Recall: 0.5000, F1: 0.5714\n",
      "rougeL: Precision: 1.0000, Recall: 0.8333, F1: 0.9091\n"
     ]
    }
   ],
   "source": [
    "reference1 = \"The cat is on the mat\"\n",
    "generated1 = \"the cat is on mat\"\n",
    "scorer4 = rouge_scorer.RougeScorer(['rouge1'],['rouge2'],['rouge3'],['rougeL'])\n",
    "\n",
    "scores = scorer.score(reference1,generated1)\n",
    "\n",
    "for rouge_type, score in scores.items():\n",
    "    print(f'{rouge_type}: Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, F1: {score.fmeasure:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced150",
   "metadata": {},
   "source": [
    "*  In the above example, the small differences like a missing word can be neglected and we can maintain a relatively high ROUGE score due to partial matches and overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e63342e0-718f-4367-a8f2-00b742c15713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Precision: 0.6875, Recall: 0.5000, F1: 0.5789\n",
      "rouge2: Precision: 0.4000, Recall: 0.2857, F1: 0.3333\n",
      "rouge3: Precision: 0.2857, Recall: 0.2000, F1: 0.2353\n",
      "rougeL: Precision: 0.6875, Recall: 0.5000, F1: 0.5789\n"
     ]
    }
   ],
   "source": [
    "reference2 = \"Cybercrime is one of the most known crime and taking money of most the people which is leading to loss of income\"\n",
    "generated2 = \"Cybercrime is one of the most prevalent crimes, affecting many individuals and leading to financial losses\" \n",
    "\n",
    "scorer4 = rouge_scorer.RougeScorer(['rouge1','rouge2','rouge3','rougeL'])\n",
    "\n",
    "scores = scorer.score(reference2,generated2)\n",
    "\n",
    "for rouge_type, score in scores.items():\n",
    "    print(f'{rouge_type}: Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, F1: {score.fmeasure:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04f145",
   "metadata": {},
   "source": [
    "* In the above example, the semantic similarity is high, but the lack of direct matches in sequences of three consecutive words leads to a low trigram ROUGE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d3e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Precision: 0.2500, Recall: 0.2727, F1: 0.2609\n",
      "rouge2: Precision: 0.0909, Recall: 0.1000, F1: 0.0952\n",
      "rouge3: Precision: 0.0000, Recall: 0.0000, F1: 0.0000\n",
      "rougeL: Precision: 0.2500, Recall: 0.2727, F1: 0.2609\n"
     ]
    }
   ],
   "source": [
    "reference3 =  \"Online crimes are increasingly common, resulting in monetary losses for victims\"\n",
    "generated3 =\" The rise in cybercrime has caused substantial financial losses for countless individuals\"\n",
    "scorer4 = rouge_scorer.RougeScorer(['rouge1'],['rouge2'],['rouge3'],['rouge4'])\n",
    "\n",
    "scores = scorer.score(reference3,generated3)\n",
    "\n",
    "for rouge_type, score in scores.items():\n",
    "    print(f'{rouge_type}: Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, F1: {score.fmeasure:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff242c82",
   "metadata": {},
   "source": [
    "* In the above example, Rogue-score mainly depends upon n gram matches and paraphrasing of the word or rewording doesn't increase the precision of the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "292c3464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n"
     ]
    }
   ],
   "source": [
    "reference4 = \"The quick brown fox jumped over the lazy dog\"\n",
    "generated4 = \"The quick brown dog jumped over the lazy fox\"\n",
    "#scorer4 = rouge_scorer.RougeScorer(['rouge1'],['rouge2'],['rouge3'],['rouge4'])\n",
    "scorer2 = rouge_scorer.RougeScorer(['rouge1'],['rouge2'])\n",
    "scores = scorer.score(reference4,generated4)\n",
    "\n",
    "for rouge_type, score in scores.items():\n",
    "    print(f'{rouge_type}: Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, F1: {score.fmeasure:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd999d",
   "metadata": {},
   "source": [
    "* In the above example, Rogue score doesn't analyze the change in text hence it provide 100% precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
