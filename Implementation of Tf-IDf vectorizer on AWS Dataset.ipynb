{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective : Performing Tf-idf Vectorizer on AWS Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import pandas as pandas\n",
    "import os \n",
    "import re \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the chunk size and read the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'overall', 'verified', 'reviewTime', 'reviewerID', 'asin',\n",
      "       'style', 'reviewerName', 'reviewText', 'summary', 'unixReviewTime',\n",
      "       'vote', 'image'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 5000\n",
    "chunks = pd.read_csv(\"E:\\\\NLP\\\\aws_review_sofware_dataset (1).csv\", sep=',', chunksize=chunk_size)\n",
    "\n",
    "# Get the first chunk and access its columns\n",
    "df = next(chunks)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chunk size matters in NLP as long data takes a lot of time to work in the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form two seperate columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words'] = \"default value\"\n",
    "df['sentences'] = \"default value\"\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    df.at[i,\"words\"] = list(\"\")\n",
    "    df.at[i,\"sentences\"] = list(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After reading the csv file make two seperate columns which should contain empty strings.\n",
    "* list(\"\") initializes words and sentences as lists of individual characters from an empty string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "      <th>words</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>A1BJHRQDYVAY2J</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Allan R. Baker</td>\n",
       "      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n",
       "      <td>ARE YOU KIDING ME?</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>APRDVZ6QBIQXT</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Lucy</td>\n",
       "      <td>This book was missing pages!!! Important pages...</td>\n",
       "      <td>missing pages!!</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>10 14, 2013</td>\n",
       "      <td>A2JZTTBSLS1QXV</td>\n",
       "      <td>0077775473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Albert V.</td>\n",
       "      <td>I have used LearnSmart and can officially say ...</td>\n",
       "      <td>Best study product out there!</td>\n",
       "      <td>1381708800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>09 19, 2005</td>\n",
       "      <td>AJ8CO6GDEE501</td>\n",
       "      <td>B00004Z0C8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Walter K. White</td>\n",
       "      <td>Compared to Magellan Mapsend the detail provid...</td>\n",
       "      <td>Garmin MapSource CD ROM</td>\n",
       "      <td>1127088000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4996</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>08 23, 2005</td>\n",
       "      <td>AHIEI9NL58TBP</td>\n",
       "      <td>B00004Z0C8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PrintGuy</td>\n",
       "      <td>First, before you buy this product, you should...</td>\n",
       "      <td>A Good Companion for Paper Topo Maps</td>\n",
       "      <td>1124755200</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4997</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>08 22, 2005</td>\n",
       "      <td>A1YA43VVH64NSO</td>\n",
       "      <td>B00004Z0C8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>George Wilson</td>\n",
       "      <td>I bought this product to be able to upload TOP...</td>\n",
       "      <td>Mediocre Mapping</td>\n",
       "      <td>1124668800</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4998</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "      <td>07 2, 2005</td>\n",
       "      <td>A3UO195ZCOA59U</td>\n",
       "      <td>B00004Z0C8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aaron D. Chacon</td>\n",
       "      <td>I break up my review into several phases:\\n\\na...</td>\n",
       "      <td>Good but there are some nagging issues</td>\n",
       "      <td>1120262400</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4999</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>06 25, 2005</td>\n",
       "      <td>A1MHW5V4GZ16PU</td>\n",
       "      <td>B00004Z0C8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bblhed</td>\n",
       "      <td>This product is what it is. It isn't a fully d...</td>\n",
       "      <td>It is a basic tool, not a precise insturment.</td>\n",
       "      <td>1119657600</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0              0      4.0      True  03 11, 2014  A240ORQ2LF9LUI  0077613252   \n",
       "1              1      4.0      True  02 23, 2014  A1YCCU0YRLS0FE  0077613252   \n",
       "2              2      1.0      True  02 17, 2014  A1BJHRQDYVAY2J  0077613252   \n",
       "3              3      3.0      True  02 17, 2014   APRDVZ6QBIQXT  0077613252   \n",
       "4              4      5.0     False  10 14, 2013  A2JZTTBSLS1QXV  0077775473   \n",
       "...          ...      ...       ...          ...             ...         ...   \n",
       "4995        4995      2.0     False  09 19, 2005   AJ8CO6GDEE501  B00004Z0C8   \n",
       "4996        4996      3.0      True  08 23, 2005   AHIEI9NL58TBP  B00004Z0C8   \n",
       "4997        4997      2.0      True  08 22, 2005  A1YA43VVH64NSO  B00004Z0C8   \n",
       "4998        4998      4.0     False   07 2, 2005  A3UO195ZCOA59U  B00004Z0C8   \n",
       "4999        4999      4.0      True  06 25, 2005  A1MHW5V4GZ16PU  B00004Z0C8   \n",
       "\n",
       "                           style         reviewerName  \\\n",
       "0     {'Format:': ' Loose Leaf'}           Michelle W   \n",
       "1     {'Format:': ' Loose Leaf'}  Rosalind White Ames   \n",
       "2     {'Format:': ' Loose Leaf'}       Allan R. Baker   \n",
       "3     {'Format:': ' Loose Leaf'}                 Lucy   \n",
       "4                            NaN            Albert V.   \n",
       "...                          ...                  ...   \n",
       "4995                         NaN      Walter K. White   \n",
       "4996                         NaN             PrintGuy   \n",
       "4997                         NaN        George Wilson   \n",
       "4998                         NaN      Aaron D. Chacon   \n",
       "4999                         NaN               bblhed   \n",
       "\n",
       "                                             reviewText  \\\n",
       "0     The materials arrived early and were in excell...   \n",
       "1     I am really enjoying this book with the worksh...   \n",
       "2     IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...   \n",
       "3     This book was missing pages!!! Important pages...   \n",
       "4     I have used LearnSmart and can officially say ...   \n",
       "...                                                 ...   \n",
       "4995  Compared to Magellan Mapsend the detail provid...   \n",
       "4996  First, before you buy this product, you should...   \n",
       "4997  I bought this product to be able to upload TOP...   \n",
       "4998  I break up my review into several phases:\\n\\na...   \n",
       "4999  This product is what it is. It isn't a fully d...   \n",
       "\n",
       "                                            summary  unixReviewTime  vote  \\\n",
       "0                                    Material Great      1394496000   NaN   \n",
       "1                                            Health      1393113600   NaN   \n",
       "2                                ARE YOU KIDING ME?      1392595200   7.0   \n",
       "3                                   missing pages!!      1392595200   3.0   \n",
       "4                     Best study product out there!      1381708800   NaN   \n",
       "...                                             ...             ...   ...   \n",
       "4995                        Garmin MapSource CD ROM      1127088000   4.0   \n",
       "4996           A Good Companion for Paper Topo Maps      1124755200  35.0   \n",
       "4997                               Mediocre Mapping      1124668800  10.0   \n",
       "4998         Good but there are some nagging issues      1120262400  26.0   \n",
       "4999  It is a basic tool, not a precise insturment.      1119657600  19.0   \n",
       "\n",
       "     image words sentences  \n",
       "0      NaN    []        []  \n",
       "1      NaN    []        []  \n",
       "2      NaN    []        []  \n",
       "3      NaN    []        []  \n",
       "4      NaN    []        []  \n",
       "...    ...   ...       ...  \n",
       "4995   NaN    []        []  \n",
       "4996   NaN    []        []  \n",
       "4997   NaN    []        []  \n",
       "4998   NaN    []        []  \n",
       "4999   NaN    []        []  \n",
       "\n",
       "[5000 rows x 15 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gkris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Punkt is a nltk tool which is required for tokenization tasks such as splitting text into sentences or words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import word and sentences tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    l1= sent_tokenize(df.loc[i,\"reviewText\"])\n",
    "    df.at[i,\"sentences\"]=l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this iteration of text in reviewText and then sentence tokenize is performed which then get stored in new columns sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gkris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Averaged_perceptron_tagger is a machine learning-based model for part-of-speech (POS) tagging, which assigns grammatical categories (e.g., noun, verb, adjective) to words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gkris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " WordNet is a large lexical database of the English language. It groups words into sets of synonyms called synsets and provides definitions, examples, and relationships between these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.utils import lemmatize_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PyWSD library, which stands for Python Word Sense Disambiguation. This function lemmatizes all words in a sentence, reducing each word to its base or root form while considering the context of the word (e.g., part of speech)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gkris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gkris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gkris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize sentences\n",
    "def lemmatize_with_nltk(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Apply the custom lemmatizer\n",
    "for k in range(df.shape[0]):\n",
    "    df.at[k, \"words\"] = []\n",
    "    for sentence in df.loc[k, \"sentences\"]:\n",
    "        lemmatized_words = lemmatize_with_nltk(sentence)\n",
    "        df.at[k, \"words\"].extend(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. WordNetLemmatizer() initializes the lemmatizer, which will be used to reduce words to their base form (e.g., \"running\" becomes \"run\").\n",
    "\n",
    "2. lemmatize_with_nltk(sentence): This function takes a sentence as input, tokenizes it into words, and then applies the lemmatizer to each token.\n",
    "   word_tokenize(sentence): Splits the sentence into individual words.\n",
    "   lemmatizer.lemmatize(word): Applies the lemmatizer to each token to get the base form of the word.\n",
    "\n",
    "3. df.shape[0] gives the number of rows in the DataFrame.\n",
    "\n",
    "4. df.at[k, \"words\"] = [] initializes an empty list in the \"words\" column for each row k.\n",
    "   \n",
    "   For each row, it accesses the \"sentences\" column (which is expected to contain a list of sentences):\n",
    "\n",
    "5. for sentence in df.loc[k, \"sentences\"]: iterates through the list of sentences in that row.\n",
    "6. lemmatize_with_nltk(sentence) calls the previously defined function to lemmatize each sentence.\n",
    "7. df.at[k, \"words\"].extend(lemmatized_words): After lemmatizing the sentence, it adds the lemmatized words to the \"words\" column\n",
    "  (using .extend() to append the list of lemmatized words).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"words_sentences\"] = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "# Iterate through the DataFrame\n",
    "for k in range(df.shape[0]):\n",
    "    words = df.loc[k, \"words\"]\n",
    "    # Check if words is empty and handle accordingly\n",
    "    if words:\n",
    "        # Join the words into a single string\n",
    "        df.loc[k, \"words_sentences\"] = functools.reduce(lambda a, b: str(a) + \" \" + str(b), words)\n",
    "    else:\n",
    "        # If the list is empty, set the column to an empty string or some default value\n",
    "        df.loc[k, \"words_sentences\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* functools is a standard library module in Python that provides higher-order functions for working with other functions. It contains several utilities that can be used to enhance, manipulate, or simplify the behavior of functions and callable objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Tf-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Term-Frequency-Inverse Document Frequency Vectorizer isa amore advanced version of CountVectorizer it highlights important words in a document ,while downweighting common words that appear in many documents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df.words_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = pd.DataFrame(tfidf_matrix.todense(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forming Dependent andf Independent features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y=df[\"verified\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As the \"verified\" column is having a textual document and as it is a target column then we can perform label encoding to encode the data for Ml algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_1 = pd.DataFrame(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_enc = df_y_1.apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   verified\n",
       "0         1\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         0\n",
       "5         1\n",
       "6         0\n",
       "7         0\n",
       "8         1\n",
       "9         1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y_enc.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 99.96%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf.fit(dense, df_y_enc)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_rf = rf.score(dense, df_y_enc)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RandomForestClassifier: This is the Random Forest algorithm from scikit-learn. Random Forest is an ensemble learning method that builds multiple decision trees during training and outputs the class that is the majority vote of the individual trees. It’s used for classification tasks.\n",
    "* accuracy_score: This function computes the accuracy of a model, i.e., the proportion of correct predictions out of all predictions. However, it's not actually used in the code directly in this case, because rf.score() is being used to calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 85.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(dense, df_y_enc)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_nb = nb.score(dense, df_y_enc)\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MultinomialNB: This is the Multinomial Naive Bayes classifier from scikit-learn, which is commonly used for classification tasks, especially for text classification (such as spam detection or sentiment analysis). It is well-suited for data where features represent counts or frequencies (e.g., word counts in a document).\n",
    "* dense: This is the feature data, which likely consists of the input features for each sample. It could be a matrix of word counts (for text data) or other types of numeric features.\n",
    "* df_y_enc: This is the target labels, which are the classes or categories that the model is trying to predict.\n",
    "* The Naive Bayes classifier computes the conditional probabilities for each feature (given each class) and then applies Bayes' theorem to predict the class labels for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC=GradientBoostingClassifier(n_estimators=1000)\n",
    "gb_c = GBC.fit(dense, df_y_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GradientBoostingClassifier: This is a class from scikit-learn that implements Gradient Boosting, an ensemble machine learning technique. Gradient Boosting builds a series of weak learners (typically decision trees), where each new tree corrects the errors made by the previous trees. This iterative process creates a powerful model capable of making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbc_score: 98.32%\n"
     ]
    }
   ],
   "source": [
    "gbc_score=GBC.score(dense, df_y_enc)\n",
    "print(f\"gbc_score: {gbc_score* 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * for 1000 rows :\n",
    "Random Forest Accuracy: 96.40%; \n",
    " Naive Bayes Accuracy: 86.40%; \n",
    " gbc_score: 96.40%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for 2500 rows :Random Forest Accuracy: 99.92%; Naive Bayes Accuracy: 78.24%; gbc_score: 98.32%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for 5000 rows :Random Forest Accuracy: 99.96%; Naive Bayes Accuracy: 85.54%; "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
