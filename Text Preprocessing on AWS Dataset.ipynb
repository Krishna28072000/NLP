{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gkris\\AppData\\Local\\Temp\\ipykernel_11620\\3157697165.py:2: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"E:\\\\NLP\\\\aws_review_sofware_dataset (1).csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(r\"E:\\\\NLP\\\\aws_review_sofware_dataset (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>A1BJHRQDYVAY2J</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Allan R. Baker</td>\n",
       "      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n",
       "      <td>ARE YOU KIDING ME?</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>APRDVZ6QBIQXT</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Lucy</td>\n",
       "      <td>This book was missing pages!!! Important pages...</td>\n",
       "      <td>missing pages!!</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>10 14, 2013</td>\n",
       "      <td>A2JZTTBSLS1QXV</td>\n",
       "      <td>0077775473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Albert V.</td>\n",
       "      <td>I have used LearnSmart and can officially say ...</td>\n",
       "      <td>Best study product out there!</td>\n",
       "      <td>1381708800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459431</th>\n",
       "      <td>459431</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>11 29, 2016</td>\n",
       "      <td>AGEWYJ2NF5C2H</td>\n",
       "      <td>B01HF41TKI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bonita Alferes</td>\n",
       "      <td>No instructions.....No Help unless you want to...</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>1480377600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459432</th>\n",
       "      <td>459432</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>03 1, 2018</td>\n",
       "      <td>A3VCFV8WEQG9R5</td>\n",
       "      <td>B01HF3G4BS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mekonen</td>\n",
       "      <td>it's a joke</td>\n",
       "      <td>One Star</td>\n",
       "      <td>1519862400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459433</th>\n",
       "      <td>459433</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>12 2, 2017</td>\n",
       "      <td>A3DXGHJF6SOHNC</td>\n",
       "      <td>B01HF3G4BS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bbeckham</td>\n",
       "      <td>I have multiple licenses of the Antivirus. I h...</td>\n",
       "      <td>This is very effective antivirus software.</td>\n",
       "      <td>1512172800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459434</th>\n",
       "      <td>459434</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>09 15, 2018</td>\n",
       "      <td>A1WOS4D7QA06DO</td>\n",
       "      <td>B01HJAMWOK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Charles E. Potter</td>\n",
       "      <td>good value</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1536969600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459435</th>\n",
       "      <td>459435</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>10 5, 2016</td>\n",
       "      <td>A20SG9ZGIIFW69</td>\n",
       "      <td>B01HJAMWOK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Joe</td>\n",
       "      <td>very nice designs easy to use.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1475625600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>459436 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  overall  verified   reviewTime      reviewerID  \\\n",
       "0                0      4.0      True  03 11, 2014  A240ORQ2LF9LUI   \n",
       "1                1      4.0      True  02 23, 2014  A1YCCU0YRLS0FE   \n",
       "2                2      1.0      True  02 17, 2014  A1BJHRQDYVAY2J   \n",
       "3                3      3.0      True  02 17, 2014   APRDVZ6QBIQXT   \n",
       "4                4      5.0     False  10 14, 2013  A2JZTTBSLS1QXV   \n",
       "...            ...      ...       ...          ...             ...   \n",
       "459431      459431      2.0      True  11 29, 2016   AGEWYJ2NF5C2H   \n",
       "459432      459432      1.0      True   03 1, 2018  A3VCFV8WEQG9R5   \n",
       "459433      459433      5.0      True   12 2, 2017  A3DXGHJF6SOHNC   \n",
       "459434      459434      5.0      True  09 15, 2018  A1WOS4D7QA06DO   \n",
       "459435      459435      5.0      True   10 5, 2016  A20SG9ZGIIFW69   \n",
       "\n",
       "              asin                       style         reviewerName  \\\n",
       "0       0077613252  {'Format:': ' Loose Leaf'}           Michelle W   \n",
       "1       0077613252  {'Format:': ' Loose Leaf'}  Rosalind White Ames   \n",
       "2       0077613252  {'Format:': ' Loose Leaf'}       Allan R. Baker   \n",
       "3       0077613252  {'Format:': ' Loose Leaf'}                 Lucy   \n",
       "4       0077775473                         NaN            Albert V.   \n",
       "...            ...                         ...                  ...   \n",
       "459431  B01HF41TKI                         NaN       Bonita Alferes   \n",
       "459432  B01HF3G4BS                         NaN              mekonen   \n",
       "459433  B01HF3G4BS                         NaN             bbeckham   \n",
       "459434  B01HJAMWOK                         NaN    Charles E. Potter   \n",
       "459435  B01HJAMWOK                         NaN                  Joe   \n",
       "\n",
       "                                               reviewText  \\\n",
       "0       The materials arrived early and were in excell...   \n",
       "1       I am really enjoying this book with the worksh...   \n",
       "2       IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...   \n",
       "3       This book was missing pages!!! Important pages...   \n",
       "4       I have used LearnSmart and can officially say ...   \n",
       "...                                                   ...   \n",
       "459431  No instructions.....No Help unless you want to...   \n",
       "459432                                        it's a joke   \n",
       "459433  I have multiple licenses of the Antivirus. I h...   \n",
       "459434                                         good value   \n",
       "459435                     very nice designs easy to use.   \n",
       "\n",
       "                                           summary  unixReviewTime vote image  \n",
       "0                                   Material Great      1394496000  NaN   NaN  \n",
       "1                                           Health      1393113600  NaN   NaN  \n",
       "2                               ARE YOU KIDING ME?      1392595200    7   NaN  \n",
       "3                                  missing pages!!      1392595200    3   NaN  \n",
       "4                    Best study product out there!      1381708800  NaN   NaN  \n",
       "...                                            ...             ...  ...   ...  \n",
       "459431                                   Two Stars      1480377600  NaN   NaN  \n",
       "459432                                    One Star      1519862400  NaN   NaN  \n",
       "459433  This is very effective antivirus software.      1512172800  NaN   NaN  \n",
       "459434                                  Five Stars      1536969600  NaN   NaN  \n",
       "459435                                  Five Stars      1475625600  NaN   NaN  \n",
       "\n",
       "[459436 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(459436, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0             0\n",
       "overall                0\n",
       "verified               0\n",
       "reviewTime             0\n",
       "reviewerID             0\n",
       "asin                   0\n",
       "style             225035\n",
       "reviewerName          76\n",
       "reviewText            86\n",
       "summary               61\n",
       "unixReviewTime         0\n",
       "vote              331583\n",
       "image             457928\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image\n",
       "['https://images-na.ssl-images-amazon.com/images/I/41TfC2-N0CL._SY88.jpg']                                                                                                                                                        6\n",
       "['https://images-na.ssl-images-amazon.com/images/I/613jr+IKO4L._SY88.jpg']                                                                                                                                                        4\n",
       "['https://images-na.ssl-images-amazon.com/images/I/51jHPTF7AHL._SY88.jpg', 'https://images-na.ssl-images-amazon.com/images/I/71QVQr49Q8L._SY88.jpg']                                                                              2\n",
       "['https://images-na.ssl-images-amazon.com/images/I/51NDxcwhVpL._SY88.jpg']                                                                                                                                                        2\n",
       "['https://images-na.ssl-images-amazon.com/images/I/81ti5uKPf8L._SY88.jpg']                                                                                                                                                        2\n",
       "                                                                                                                                                                                                                                 ..\n",
       "['https://images-na.ssl-images-amazon.com/images/I/61DUcDYYMQL._SY88.jpg']                                                                                                                                                        1\n",
       "['https://images-na.ssl-images-amazon.com/images/I/8119Lyw+WIL._SY88.jpg', 'https://images-na.ssl-images-amazon.com/images/I/819ElvjLPCL._SY88.jpg', 'https://images-na.ssl-images-amazon.com/images/I/81DvX-3PSJL._SY88.jpg']    1\n",
       "['https://images-na.ssl-images-amazon.com/images/I/71opk6liD5L._SY88.jpg']                                                                                                                                                        1\n",
       "['https://images-na.ssl-images-amazon.com/images/I/81d0ta3CqmL._SY88.jpg', 'https://images-na.ssl-images-amazon.com/images/I/81jsPXBzU+L._SY88.jpg']                                                                              1\n",
       "['https://images-na.ssl-images-amazon.com/images/I/7162r4xfsvL._SY88.jpg']                                                                                                                                                        1\n",
       "Name: count, Length: 1474, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['image'].unique()\n",
    "df['image'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allan R. Baker</td>\n",
       "      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n",
       "      <td>ARE YOU KIDING ME?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lucy</td>\n",
       "      <td>This book was missing pages!!! Important pages...</td>\n",
       "      <td>missing pages!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albert V.</td>\n",
       "      <td>I have used LearnSmart and can officially say ...</td>\n",
       "      <td>Best study product out there!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>piepiepie75</td>\n",
       "      <td>My first experience with Human Japanese was th...</td>\n",
       "      <td>Better than the Human Japanese 1...but not muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>K. Grier</td>\n",
       "      <td>This is the first language software that I hav...</td>\n",
       "      <td>Great Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>H. Granat</td>\n",
       "      <td>Human japanese is the best pc program for lear...</td>\n",
       "      <td>Love it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Kenneth S. French</td>\n",
       "      <td>If you want to learn or teach Japanese I would...</td>\n",
       "      <td>Outstanding, easy-to-use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Lee W. Hericks</td>\n",
       "      <td>Human Japanese has an elegant, yet simple user...</td>\n",
       "      <td>Learning a foreign language with a human touch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerName                                         reviewText  \\\n",
       "0            Michelle W  The materials arrived early and were in excell...   \n",
       "1   Rosalind White Ames  I am really enjoying this book with the worksh...   \n",
       "2        Allan R. Baker  IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...   \n",
       "3                  Lucy  This book was missing pages!!! Important pages...   \n",
       "4             Albert V.  I have used LearnSmart and can officially say ...   \n",
       "..                  ...                                                ...   \n",
       "95          piepiepie75  My first experience with Human Japanese was th...   \n",
       "96             K. Grier  This is the first language software that I hav...   \n",
       "97            H. Granat  Human japanese is the best pc program for lear...   \n",
       "98    Kenneth S. French  If you want to learn or teach Japanese I would...   \n",
       "99       Lee W. Hericks  Human Japanese has an elegant, yet simple user...   \n",
       "\n",
       "                                              summary  \n",
       "0                                      Material Great  \n",
       "1                                              Health  \n",
       "2                                  ARE YOU KIDING ME?  \n",
       "3                                     missing pages!!  \n",
       "4                       Best study product out there!  \n",
       "..                                                ...  \n",
       "95  Better than the Human Japanese 1...but not muc...  \n",
       "96                                      Great Product  \n",
       "97                                           Love it!  \n",
       "98                           Outstanding, easy-to-use  \n",
       "99     Learning a foreign language with a human touch  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'E:\\\\NLP\\\\aws_review_sofware_dataset (1).csv', usecols=[ 'reviewerName', 'reviewText', 'summary'], nrows=100)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewerName'] = df['reviewerName'].str.lower()\n",
    "df['reviewText'] = df['reviewText'].str.lower()\n",
    "df['summary'] = df['summary'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewerName'] = df['reviewerName'].apply(remove_html_tags)\n",
    "df['reviewText'] = df['reviewText'].apply(remove_html_tags)\n",
    "df['summary'] = df['summary'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www.\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText'] = df['reviewText'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michelle w</td>\n",
       "      <td>the materials arrived early and were in excell...</td>\n",
       "      <td>material great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rosalind white ames</td>\n",
       "      <td>i am really enjoying this book with the worksh...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allan r. baker</td>\n",
       "      <td>if you are taking this class don\"t waste your ...</td>\n",
       "      <td>are you kiding me?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lucy</td>\n",
       "      <td>this book was missing pages!!! important pages...</td>\n",
       "      <td>missing pages!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert v.</td>\n",
       "      <td>i have used learnsmart and can officially say ...</td>\n",
       "      <td>best study product out there!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>piepiepie75</td>\n",
       "      <td>my first experience with human japanese was th...</td>\n",
       "      <td>better than the human japanese 1...but not muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>k. grier</td>\n",
       "      <td>this is the first language software that i hav...</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>h. granat</td>\n",
       "      <td>human japanese is the best pc program for lear...</td>\n",
       "      <td>love it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>kenneth s. french</td>\n",
       "      <td>if you want to learn or teach japanese i would...</td>\n",
       "      <td>outstanding, easy-to-use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>lee w. hericks</td>\n",
       "      <td>human japanese has an elegant, yet simple user...</td>\n",
       "      <td>learning a foreign language with a human touch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerName                                         reviewText  \\\n",
       "0            michelle w  the materials arrived early and were in excell...   \n",
       "1   rosalind white ames  i am really enjoying this book with the worksh...   \n",
       "2        allan r. baker  if you are taking this class don\"t waste your ...   \n",
       "3                  lucy  this book was missing pages!!! important pages...   \n",
       "4             albert v.  i have used learnsmart and can officially say ...   \n",
       "..                  ...                                                ...   \n",
       "95          piepiepie75  my first experience with human japanese was th...   \n",
       "96             k. grier  this is the first language software that i hav...   \n",
       "97            h. granat  human japanese is the best pc program for lear...   \n",
       "98    kenneth s. french  if you want to learn or teach japanese i would...   \n",
       "99       lee w. hericks  human japanese has an elegant, yet simple user...   \n",
       "\n",
       "                                              summary  \n",
       "0                                      material great  \n",
       "1                                              health  \n",
       "2                                  are you kiding me?  \n",
       "3                                     missing pages!!  \n",
       "4                       best study product out there!  \n",
       "..                                                ...  \n",
       "95  better than the human japanese 1...but not muc...  \n",
       "96                                      great product  \n",
       "97                                           love it!  \n",
       "98                           outstanding, easy-to-use  \n",
       "99     learning a foreign language with a human touch  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText'] = df['reviewText'].apply(remove_punc)\n",
    "df['summary'] = df['summary'].apply(remove_punc)\n",
    "df['reviewerName'] = df['reviewerName'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michelle w</td>\n",
       "      <td>materials arrived early excellent condition ho...</td>\n",
       "      <td>material great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rosalind white ames</td>\n",
       "      <td>really enjoying book worksheets make review go...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allan r baker</td>\n",
       "      <td>taking class dont waste money called book 1400...</td>\n",
       "      <td>kiding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lucy</td>\n",
       "      <td>book missing pages important pages couldnt ans...</td>\n",
       "      <td>missing pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert v</td>\n",
       "      <td>used learnsmart officially say amazing study t...</td>\n",
       "      <td>best study product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>piepiepie75</td>\n",
       "      <td>first experience human japanese first version ...</td>\n",
       "      <td>better     human japanese 1but   much changed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>k grier</td>\n",
       "      <td>first language software purchased love way inf...</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>h granat</td>\n",
       "      <td>human japanese best pc program learning japane...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>kenneth s french</td>\n",
       "      <td>want learn teach japanese would highly recomme...</td>\n",
       "      <td>outstanding easytouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>lee w hericks</td>\n",
       "      <td>human japanese elegant yet simple user interfa...</td>\n",
       "      <td>learning   foreign language     human touch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerName                                         reviewText  \\\n",
       "0            michelle w  materials arrived early excellent condition ho...   \n",
       "1   rosalind white ames  really enjoying book worksheets make review go...   \n",
       "2         allan r baker  taking class dont waste money called book 1400...   \n",
       "3                  lucy  book missing pages important pages couldnt ans...   \n",
       "4              albert v  used learnsmart officially say amazing study t...   \n",
       "..                  ...                                                ...   \n",
       "95          piepiepie75  first experience human japanese first version ...   \n",
       "96              k grier  first language software purchased love way inf...   \n",
       "97             h granat  human japanese best pc program learning japane...   \n",
       "98     kenneth s french  want learn teach japanese would highly recomme...   \n",
       "99        lee w hericks  human japanese elegant yet simple user interfa...   \n",
       "\n",
       "                                          summary  \n",
       "0                                  material great  \n",
       "1                                          health  \n",
       "2                                        kiding    \n",
       "3                                   missing pages  \n",
       "4                          best study product      \n",
       "..                                            ...  \n",
       "95  better     human japanese 1but   much changed  \n",
       "96                                  great product  \n",
       "97                                         love    \n",
       "98                          outstanding easytouse  \n",
       "99    learning   foreign language     human touch  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removal of Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append(' ')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return\" \".join(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText'] = df['reviewText'].apply(remove_stopwords)\n",
    "df['summary'] = df['summary'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michelle w</td>\n",
       "      <td>materials arrived early excellent condition ho...</td>\n",
       "      <td>material great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rosalind white ames</td>\n",
       "      <td>really enjoying book worksheets make review go...</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allan r baker</td>\n",
       "      <td>taking class dont waste money called book 1400...</td>\n",
       "      <td>kiding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lucy</td>\n",
       "      <td>book missing pages important pages couldnt ans...</td>\n",
       "      <td>missing pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert v</td>\n",
       "      <td>used learnsmart officially say amazing study t...</td>\n",
       "      <td>best study product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>piepiepie75</td>\n",
       "      <td>first experience human japanese first version ...</td>\n",
       "      <td>better human japanese 1but much changed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>k grier</td>\n",
       "      <td>first language software purchased love way inf...</td>\n",
       "      <td>great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>h granat</td>\n",
       "      <td>human japanese best pc program learning japane...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>kenneth s french</td>\n",
       "      <td>want learn teach japanese would highly recomme...</td>\n",
       "      <td>outstanding easytouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>lee w hericks</td>\n",
       "      <td>human japanese elegant yet simple user interfa...</td>\n",
       "      <td>learning foreign language human touch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerName                                         reviewText  \\\n",
       "0            michelle w  materials arrived early excellent condition ho...   \n",
       "1   rosalind white ames  really enjoying book worksheets make review go...   \n",
       "2         allan r baker  taking class dont waste money called book 1400...   \n",
       "3                  lucy  book missing pages important pages couldnt ans...   \n",
       "4              albert v  used learnsmart officially say amazing study t...   \n",
       "..                  ...                                                ...   \n",
       "95          piepiepie75  first experience human japanese first version ...   \n",
       "96              k grier  first language software purchased love way inf...   \n",
       "97             h granat  human japanese best pc program learning japane...   \n",
       "98     kenneth s french  want learn teach japanese would highly recomme...   \n",
       "99        lee w hericks  human japanese elegant yet simple user interfa...   \n",
       "\n",
       "                                    summary  \n",
       "0                            material great  \n",
       "1                                    health  \n",
       "2                                    kiding  \n",
       "3                             missing pages  \n",
       "4                        best study product  \n",
       "..                                      ...  \n",
       "95  better human japanese 1but much changed  \n",
       "96                            great product  \n",
       "97                                     love  \n",
       "98                    outstanding easytouse  \n",
       "99    learning foreign language human touch  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"u\\U0001F600-\\U0001F64F\"  # Emotions\n",
    "                               \"u\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                               \"u\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                               \"u\\U0001F1E0-\\U0001F1FF\"  # Flags(iOS)\n",
    "                               \"u\\U00002702-\\U000027B0\"\n",
    "                               \"u\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText'] = df['reviewText'].apply(remove_emoji)\n",
    "df['summary'] = df['summary'].apply(remove_emoji)\n",
    "df['reviewerName'] =df['reviewerName'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_columns = [ 'reviewText', 'summary']\n",
    "\n",
    "for col in tokenized_columns:\n",
    "    df[col] = df[col].apply(lambda x: word_tokenize(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michelle w</td>\n",
       "      <td>[materials, arrived, early, excellent, conditi...</td>\n",
       "      <td>[material, great]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rosalind white ames</td>\n",
       "      <td>[really, enjoying, book, worksheets, make, rev...</td>\n",
       "      <td>[health]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allan r baker</td>\n",
       "      <td>[taking, class, dont, waste, money, called, bo...</td>\n",
       "      <td>[kiding]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lcy</td>\n",
       "      <td>[book, missing, pages, important, pages, coldn...</td>\n",
       "      <td>[missing, pages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert v</td>\n",
       "      <td>[sed, learnsmart, officially, say, amazing, st...</td>\n",
       "      <td>[best, stdy, prodct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>piepiepie75</td>\n",
       "      <td>[first, experience, hman, japanese, first, ver...</td>\n",
       "      <td>[better, hman, japanese, 1bt, mch, changed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>k grier</td>\n",
       "      <td>[first, langage, software, prchased, love, way...</td>\n",
       "      <td>[great, prodct]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>h granat</td>\n",
       "      <td>[hman, japanese, best, pc, program, learning, ...</td>\n",
       "      <td>[love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>kenneth s french</td>\n",
       "      <td>[want, learn, teach, japanese, wold, highly, r...</td>\n",
       "      <td>[otstanding, easytose]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>lee w hericks</td>\n",
       "      <td>[hman, japanese, elegant, yet, simple, ser, in...</td>\n",
       "      <td>[learning, foreign, langage, hman, toch]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerName                                         reviewText  \\\n",
       "0            michelle w  [materials, arrived, early, excellent, conditi...   \n",
       "1   rosalind white ames  [really, enjoying, book, worksheets, make, rev...   \n",
       "2         allan r baker  [taking, class, dont, waste, money, called, bo...   \n",
       "3                   lcy  [book, missing, pages, important, pages, coldn...   \n",
       "4              albert v  [sed, learnsmart, officially, say, amazing, st...   \n",
       "..                  ...                                                ...   \n",
       "95          piepiepie75  [first, experience, hman, japanese, first, ver...   \n",
       "96              k grier  [first, langage, software, prchased, love, way...   \n",
       "97             h granat  [hman, japanese, best, pc, program, learning, ...   \n",
       "98     kenneth s french  [want, learn, teach, japanese, wold, highly, r...   \n",
       "99        lee w hericks  [hman, japanese, elegant, yet, simple, ser, in...   \n",
       "\n",
       "                                        summary  \n",
       "0                             [material, great]  \n",
       "1                                      [health]  \n",
       "2                                      [kiding]  \n",
       "3                              [missing, pages]  \n",
       "4                          [best, stdy, prodct]  \n",
       "..                                          ...  \n",
       "95  [better, hman, japanese, 1bt, mch, changed]  \n",
       "96                              [great, prodct]  \n",
       "97                                       [love]  \n",
       "98                       [otstanding, easytose]  \n",
       "99     [learning, foreign, langage, hman, toch]  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [materials, arrived, early, excellent, conditi...\n",
       "1     [really, enjoying, book, worksheets, make, rev...\n",
       "2     [taking, class, dont, waste, money, called, bo...\n",
       "3     [book, missing, pages, important, pages, coldn...\n",
       "4     [sed, learnsmart, officially, say, amazing, st...\n",
       "                            ...                        \n",
       "95    [first, experience, hman, japanese, first, ver...\n",
       "96    [first, langage, software, prchased, love, way...\n",
       "97    [hman, japanese, best, pc, program, learning, ...\n",
       "98    [want, learn, teach, japanese, wold, highly, r...\n",
       "99    [hman, japanese, elegant, yet, simple, ser, in...\n",
       "Name: reviewText, Length: 100, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['materials',\n",
       " 'arrived',\n",
       " 'early',\n",
       " 'excellent',\n",
       " 'condition',\n",
       " 'however',\n",
       " 'money',\n",
       " 'spent',\n",
       " 'really',\n",
       " 'sholdve',\n",
       " 'come',\n",
       " 'binder',\n",
       " 'loose',\n",
       " 'leaf']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviewText'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               [material, great]\n",
       "1                                        [health]\n",
       "2                                        [kiding]\n",
       "3                                [missing, pages]\n",
       "4                            [best, stdy, prodct]\n",
       "                         ...                     \n",
       "95    [better, hman, japanese, 1bt, mch, changed]\n",
       "96                                [great, prodct]\n",
       "97                                         [love]\n",
       "98                         [otstanding, easytose]\n",
       "99       [learning, foreign, langage, hman, toch]\n",
       "Name: summary, Length: 100, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "materials,arrived,early,excellent,condition, however,money, spent,really,sholdve,come,binder,loose,leaf\n"
     ]
    }
   ],
   "source": [
    "text = 'materials,arrived,early,excellent,condition, however,money, spent,really,sholdve,come,binder,loose,leaf'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'materials,arrived,early,excellent,condition, however,money, spent,really,sholdve,come,binder,loose,leaf'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'strong backgroung, good read, quit up to date. it take a holist approach to the subject. lack of refer is a bit surprising.jorg'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Strong backgroung, good read, quite up to date. It takes a holistic approach to the subject. Lack of references is a bit surprising.Jorge\"\n",
    "stem_words(text1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gkris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\gkris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the WordNet corpus\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# You might also need to download the 'omw-1.4' package for wordnet\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "aws                 aws                 \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He aws running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations = \"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "    \n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
